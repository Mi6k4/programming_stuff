Apache Spark — это фреймворк с открытым исходным кодом, который в основном используется для анализа Big Data

Что Такое Ленивые Вычисления’?
Как становится понятно из названия, такой тип вычислений откладывается до того момента, пока не понадобится значение предмета. 
Кроме того, ленивые вычисления выполняются лишь один раз – повторных вычислений не происходит.

Для Чего Используется SparkCore?
SparkCore — это главный движок, отвечающий за все процессы внутри Spark.
 Учитывая сказанное, вы, скорее всего, не будете удивлены тем, что он имеет множество важных обязанностей – мониторинг, управление памятью и хранилищем, 
планирование задач и многое другое



о RDD: Resilient Distributed Dataset. По сути это надежная распределенная таблица 
(на самом деле RDD содержит произвольную коллекцию, но удобнее всего работать с кортежами, как в реляционной таблице). 
RDD может быть полностью виртуальной и просто знать, как она породилась, чтобы, например, в случае сбоя узла, восстановиться.
 А может быть и материализована — распределенно, в памяти или на диске (или в памяти с вытеснением на диск).
 Также, вну
три, RDD разбита на партиции — это минимальный объем RDD, который будет обработан каждым рабочим узлом.



RDD – это распределенная коллекция данных, расположенных по нескольким узлам кластера, набор объектов Java или Scala, представляющих данные. 
RDD работает со структурированными и с неструктурированные данными. 
Также, как DataFrame и DataSet, RDD не выводит схему загруженных данных и требует от пользователя ее указания.

DataFrame – это распределенная коллекция данных в виде именованных столбцов, аналогично таблице в реляционной базе данных. 
DataFrame работает только со структурированными и полуструктурированными данными, организуя информацию по столбцам, как в реляционных таблицах. 
Это позволяет Spark управлять схемой данных.

DataSet – это расширение API DataFrame, обеспечивающее функциональность объектно-ориентированного RDD-API (строгая типизация, лямбда-функции),
 производительность оптимизатора запросов Catalyst и механизм хранения вне кучи. DataSet эффективно обрабатывает структурированные и неструктурированные данные,
 представляя их в виде строки JVM-объектов или коллекции. Для представления табличных форм используется кодировщик (encoder).




Преобразования - это функции, выполняемые по требованию для создания нового RDD. 
За всеми преобразованиями следуют действия. Некоторые примеры преобразований включают map, filter и reduceByKey.

Действия - это результаты вычислений или преобразований RDD. 
После выполнения действия данные из RDD возвращаются на локальный компьютер. Некоторые примеры действий включают сокращение, сбор, первый и принятие.




преобразования (transformations) – отложенные или ленивые вычисления, которые фактически не выполняются сразу, а после материализации запроса и вызове какого-либо действия. 
При этом создается план запроса, но сами данные все еще находятся в хранилище и ожидают обработки.
действия (actions) – функции, запрашивающие вывод. При этом не только создается план запроса, но и оптимизируется оптимизатором Spark, 
а также физический план компилируется в RDD DAG, который делится на этапы (stages) и задачи (tasks), выполняемые в кластере. 
Оптимизированный план запроса генерирует высокоэффективный Java-код, который работает с внутренним представлением данных в формате Tungsten [2].








map  принимает на вход список значений и некую функцию, которую затем применяет к каждому элементу списка и возвращает новый список;

reduce (свёртка) — преобразует список к единственному атомарному значению при помощи заданной функции, 
которой на каждой итерации передаются новый элемент списка и промежуточный результат.
Для обработки данных в соответствии с вычислительной моделью MapReduce следует определить обе эти функции, 
указать имена входных и выходных файлов, а также параметры обработки.

Сама вычислительная модель состоит из 3-хшаговой комбинации вышеприведенных функций [2]:

Map – предварительная обработка входных данных в виде большого список значений. 
При этом главный узел кластера (master node) получает этот список, делит его на части и передает рабочим узлам (worker node). 
Далее каждый рабочий узел применяет функцию Map к локальным данным и записывает результат в формате «ключ-значение» во временное хранилище.

Shuffle, когда рабочие узлы перераспределяют данные на основе ключей, ранее созданных функцией Map, таким образом, чтобы все данные одного ключа лежали на одном рабочем узле.

Reduce – параллельная обработка каждым рабочим узлом каждой группы данных по порядку следования ключей и «склейка» результатов на master node. 
Главный узел получает промежуточные ответы от рабочих узлов и передаёт их на свободные узлы для выполнения следующего шага.
 Получившийся после прохождения всех необходимых шагов результат – это и есть решение исходной задачи.


10. How do we create RDDs in Spark?
Spark provides two methods to create RDD:

1. By parallelizing a collection in your Driver program.

2. This makes use of SparkContext’s ‘parallelize’

1
2
3
method val DataArray = Array(2,4,6,8,10)
 
val DataRDD = sc.parallelize(DataArray)
3. By loading an external dataset from external storage like HDFS, HBase, shared file system.



litteral view Для множественного использование генератора

Coalescing Partitions снижает количество партиций
codeGen - почитать

save as table and save in spark

бакетирование - почитать